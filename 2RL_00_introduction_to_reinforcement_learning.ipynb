{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aditya-r-m/notes/blob/main/2RL_00_introduction_to_reinforcement_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Agent & Environment\n",
        "\n",
        "At timestep $t$,\n",
        "\n",
        "- Agent receives observation $O_t$ and reward $R_t$.\n",
        "- Agent executes action $A_t$\n",
        "- Environment receives action $A_t$\n",
        "- Environment emits observation $O_{t+1}$ and reward $R_{t+t}$\n",
        "\n",
        "The observations, rewards, and allowed actions depend on the internal state $U_t \\supseteq O_t$ of the environment. The state transitions can be deterministic or stochastic.\n",
        "\n",
        "History is the full sequence of observations, actions, and rewards,\n",
        "\n",
        "$H_t =  O_0, A_0, R_1,..,A_{t-1},R_t,O_t$\n",
        "\n",
        "The agent state $S_t$ is generally some compression of $H_t$."
      ],
      "metadata": {
        "id": "4xnxmaNP_C72"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MDP\n",
        "\n",
        "A decision process is Markovian if the probability of a getting a given (reward, state) doesn't change if we add more history,\n",
        "\n",
        "$p(r, s | S_t, A_t) = p(r, s | H_t, A_t)$\n",
        "\n",
        "Two extreme cases where the process is Markovian are,\n",
        "1. If observations are completely uninformative\n",
        "2. If observations include full environment state\n",
        "\n",
        "Observations can be partial, thus, not Markovian. Decision processes on partially-observable systems are called POMDP.\n"
      ],
      "metadata": {
        "id": "n7wfl0xbL52v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reward hypothesis\n",
        "\n",
        "Any goal can be formalized as the outcome of maximizing a cumulative reward - where reward $R_t$ is a scalar feedback signal on some action $a_t$.\n",
        "\n",
        "The cumulative reward is called the return,\n",
        "\n",
        "$G_t = \\sum_{i=0}^\\infty \\gamma^i R_{t+1+i} = R_t + \\gamma G_{t+1}$\n",
        "\n",
        "where $\\gamma \\in [0,1)$ is the discount factor for distinguishing long-term vs short-term rewards."
      ],
      "metadata": {
        "id": "Y4keMpbSBWxH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Values\n",
        "\n",
        "The expected cumulative reward from a state $s$ is called the value,\n",
        "\n",
        "$v(s) = E[G_t | S_t = s] = E[R_{t+1} + \\gamma v(S_{t+1}) | S_t = s]$\n",
        "\n",
        "The goal is to maximize value by picking suitable actions.\n",
        "\n",
        "Value can also be conditioned on actions,\n",
        "\n",
        "$q(s, a) = E[G_t | S_t = s, A_t = a]$"
      ],
      "metadata": {
        "id": "6ogJkJbyCfEu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Agent Components\n",
        "\n",
        "#### Agent State\n",
        "constructed from history, $S_{t+1} = u(S_t, A_t, R_{t+1}, O_{t+1})$ where $u$ is the state update function.\n",
        "\n",
        "#### Policy\n",
        "mapping from states to actions which can either be Determinisitc $A = \\pi(S)$, or Stochastic $p(A|S) = \\pi(A|S)$.\n",
        "\n",
        "#### Value functions\n",
        "Depending on the [current] agent policy, we can define value function as,\n",
        "\n",
        "$\n",
        "v_{\\pi}(s) = E[R_{t+1} + \\gamma v_{\\pi}(S_t+1) | S_t = s, A_t \\leftarrow \\pi(s)]\n",
        "$\n",
        "\n",
        "The optimal value function can be defined as,\n",
        "\n",
        "$\n",
        "v_*(s) = \\max_a E[R_{t+1} + \\gamma v_*(S_{t+1}) | S_t = s, A_t = a]\n",
        "$\n",
        "\n",
        "Agents often approximate the value functions, suitable value functions allow us to behave well, even in intractably large domains.\n",
        "\n",
        "#### Model\n",
        "\n",
        "Predicts what the environement will do next,\n",
        "\n",
        "$P(s, a, s') = p(S_{t+1} = s' | S_t = s, A_t = a)$\n",
        "$R(s, a) = E[R_{t+1} | S_t = s, A_t = a]$\n",
        "\n",
        "A Model does not give us a good policy, we would still need to plan."
      ],
      "metadata": {
        "id": "B78dh5AJFN1W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Agent Categories\n",
        "\n",
        "#### Value Based\n",
        "\n",
        "Policy is implicit, agent learn value function & picks the action leading to the highest return.\n",
        "\n",
        "#### Policy Based\n",
        "\n",
        "Policy can also be learnt without learning value functions.\n",
        "\n",
        "#### Actor Critic\n",
        "\n",
        "Planning is done both using the Actor-part (policy) and Learning a Value Function (critic) that updates this policy in some way."
      ],
      "metadata": {
        "id": "6R21D3nuR7C9"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}