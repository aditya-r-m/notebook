{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM84nQAqnWRwlq64bHxaSiN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aditya-r-m/notes/blob/main/2RL_01_exploration_and_control.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exploration** is the process of learning the environment structure better by trying out actions with potentially low known return, but a high degree of uncertainity.\n",
        "\n",
        "**Exploitation** is the process of making actions which are known to be optimal to gather maximal cumulative reward."
      ],
      "metadata": {
        "id": "qQCLHGj6wm9V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi-Armed Bandit\n",
        "\n",
        "We have a set of distributions $\\{R_a | a \\in A\\}$ where $A$ is known set of Actions (Arms). $R_a$ is a distribution on Rewards, given action $a$\n",
        "\n",
        "The agent selects an action $A_t \\in A$ at each timestep $t$ & the environment generates a reward $R_t \\sim R_{A_t}$\n",
        "\n",
        "The goal is to maximize cumulative reward $\\sum_{i=1}^t R_i$\n",
        "\n",
        "We do this by learning a policy, which is a distribution on $A$."
      ],
      "metadata": {
        "id": "XOkEOlByGX9W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Valus & Regret\n",
        "\n",
        "The action value for action $a$ is the expected reward $q(a) = E[R_t | A_t = a]$\n",
        "\n",
        "The optimal value is $v_* = \\max_{a \\in A} q(a) = \\max_a E[R_t | A_t = a]$\n",
        "\n",
        "Regret of an action $a$ is $\\Delta_a = v_* - q(a)$ which is zero for optimal action.\n",
        "\n",
        "The problem of maximizing cumulative reward is identical to minizing total regret $L_t = \\sum_{n=1}^t \\Delta_{A_n}$ and Good policies will accumulate bounded regret.\n"
      ],
      "metadata": {
        "id": "R6mG_YpYH6VO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Algorithms\n",
        "\n",
        "There are several algorithms to approach the multi-armed bandit problem,\n",
        "\n",
        "- Greedy\n",
        "- $\\epsilon$-Greedy\n",
        "- UCB\n",
        "- Thompson sampling\n",
        "- Policy gradients\n",
        "\n",
        "The first three all use action value estimates $Q_t(a) \\approx q(a)$\n"
      ],
      "metadata": {
        "id": "5XLsIm_HKL8r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Action Values estimation\n",
        "\n",
        "A simple estimate for $q(a) = E[R_t | A_t = a]$ is the average of sampled rewards,\n",
        "\n",
        "$\n",
        "Q_t(a) = \\frac{\\sum_{n=1}^t I(A_n = a) R_n}{\\sum_{n=1}^t I(A_n = a)}\n",
        "$\n",
        "\n",
        "where $I(\\cdot)$ is the indicator function : $I(True) = 1$ and $I(False) = 0$ and the count for an action $a$ is\n",
        "\n",
        "$\n",
        "N_t(a) = \\sum_{n=1}^t I(A_n = a)\n",
        "$\n",
        "\n",
        "This average can be learnt iteratively as,\n",
        "\n",
        "$\n",
        "Q_t(A_t)\n",
        "= \\left[1 - \\frac{1}{N_t(A_t)}\\right] Q_{t-1}(A_t) + \\left[\\frac{1}{N_t(A_t)}\\right] R_t\n",
        "= Q_{t-1} + \\frac{1}{N_t(A_t)} \\left[R_t - Q_{t-1}(A_t)\\right]\n",
        "$,\n",
        "\n",
        "$\n",
        "\\forall a \\neq A_t, Q_t(a) = Q_{t-1}(a)\n",
        "$\n",
        "\n",
        "where $N_0(a) = 0$\n",
        "\n",
        "Note that the step-size $\\alpha_t = \\frac{1}{N_t(A_t)}$ which is mulitplied to the error term above leads to averaging, but other settings are also possible. For example, a constant step-size will lead to tracking instead of averaging - which may be used when the problem is non-stationary in some way."
      ],
      "metadata": {
        "id": "AqADcWP-LI9g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Greedy and $\\epsilon$-Greedy\n",
        "\n",
        "Greedy approach can get stuck on a suboptimal actions, leading to linear expected total regret.\n",
        "\n",
        "$\\epsilon$-Greedy selects greedy action with probability $(1 - \\epsilon)$ : $a = argmax_{a \\in A} Q_t(a)$ and random action otherwise.\n",
        "\n",
        "$\n",
        "\\pi_t(a) = \\left\\{\n",
        "\\begin{array}{ll}\n",
        "      (1 - \\epsilon) + \\epsilon/|A| & \\text{if} Q_t(a) = max_b Q_t(b) \\\\\n",
        "        \\epsilon/|A| & \\text{otherwise} \\\\\n",
        "\\end{array}\n",
        "\\right.\n",
        "$\n",
        "\n",
        "$\\epsilon$-greedy is a popular choice. Though with a constant $\\epsilon$ it continues to explore, so it still leads to linear expected total regret."
      ],
      "metadata": {
        "id": "IXgjHq1VQROV"
      }
    }
  ]
}