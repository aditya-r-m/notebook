{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMuTFx+CvbQ4dHl4xOAv7o1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aditya-r-m/notes/blob/main/2RL_01_exploration_and_control.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exploration** is the process of learning the environment structure better by trying out actions with potentially low known return, but a high degree of uncertainity.\n",
        "\n",
        "**Exploitation** is the process of making actions which are known to be optimal to gather maximal cumulative reward."
      ],
      "metadata": {
        "id": "qQCLHGj6wm9V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi-Armed Bandit\n",
        "\n",
        "We have a set of distributions $\\{R_a | a \\in A\\}$ where $A$ is known set of Actions (Arms). $R_a$ is a distribution on Rewards, given action $a$\n",
        "\n",
        "The agent selects an action $A_t \\in A$ at each timestep $t$ & the environment generates a reward $R_t \\sim R_{A_t}$\n",
        "\n",
        "The goal is to maximize cumulative reward $\\sum_{i=1}^t R_i$\n",
        "\n",
        "We do this by learning a policy, which is a distribution on $A$."
      ],
      "metadata": {
        "id": "XOkEOlByGX9W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Valus & Regret\n",
        "\n",
        "The action value for action $a$ is the expected reward $q(a) = E[R_t | A_t = a]$\n",
        "\n",
        "The optimal value is $v_* = \\max_{a \\in A} q(a) = \\max_a E[R_t | A_t = a]$\n",
        "\n",
        "Regret of an action $a$ is $\\Delta_a = v_* - q(a)$ which is zero for optimal action.\n",
        "\n",
        "The problem of maximizing cumulative reward is identical to minizing total regret $L_t = \\sum_{n=1}^t \\Delta_{A_n}$ and Good policies will accumulate bounded regret.\n"
      ],
      "metadata": {
        "id": "R6mG_YpYH6VO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Algorithms\n",
        "\n",
        "There are several algorithms to approach the multi-armed bandit problem,\n",
        "\n",
        "- Greedy\n",
        "- $\\epsilon$-Greedy\n",
        "- UCB\n",
        "- Thompson sampling\n",
        "- Policy gradients\n",
        "\n",
        "The first three all use action value estimates $Q_t(a) \\approx q(a)$\n"
      ],
      "metadata": {
        "id": "5XLsIm_HKL8r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Action Values estimation\n",
        "\n",
        "A simple estimate for $q(a) = E[R_t | A_t = a]$ is the average of sampled rewards,\n",
        "\n",
        "$\n",
        "Q_t(a) = \\frac{\\sum_{n=1}^t I(A_n = a) R_n}{\\sum_{n=1}^t I(A_n = a)}\n",
        "$\n",
        "\n",
        "where $I(\\cdot)$ is the indicator function : $I(True) = 1$ and $I(False) = 0$ and the count for an action $a$ is\n",
        "\n",
        "$\n",
        "N_t(a) = \\sum_{n=1}^t I(A_n = a)\n",
        "$\n",
        "\n",
        "This average can be learnt iteratively as,\n",
        "\n",
        "$\n",
        "Q_t(A_t)\n",
        "= \\left[1 - \\frac{1}{N_t(A_t)}\\right] Q_{t-1}(A_t) + \\left[\\frac{1}{N_t(A_t)}\\right] R_t\n",
        "= Q_{t-1} + \\frac{1}{N_t(A_t)} \\left[R_t - Q_{t-1}(A_t)\\right]\n",
        "$,\n",
        "\n",
        "$\n",
        "\\forall a \\neq A_t, Q_t(a) = Q_{t-1}(a)\n",
        "$\n",
        "\n",
        "where $N_0(a) = 0$\n",
        "\n",
        "Note that the step-size $\\alpha_t = \\frac{1}{N_t(A_t)}$ which is mulitplied to the error term above leads to averaging, but other settings are also possible. For example, a constant step-size will lead to tracking instead of averaging - which may be used when the problem is non-stationary in some way."
      ],
      "metadata": {
        "id": "AqADcWP-LI9g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Greedy and $\\epsilon$-Greedy\n",
        "\n",
        "Greedy approach can get stuck on a suboptimal actions, leading to linear expected total regret.\n",
        "\n",
        "$\\epsilon$-Greedy selects greedy action with probability $(1 - \\epsilon)$ : $a = argmax_{a \\in A} Q_t(a)$ and random action otherwise.\n",
        "\n",
        "$\n",
        "\\pi_t(a) = \\left\\{\n",
        "\\begin{array}{ll}\n",
        "      (1 - \\epsilon) + \\epsilon/|A| & \\text{if} Q_t(a) = max_b Q_t(b) \\\\\n",
        "        \\epsilon/|A| & \\text{otherwise} \\\\\n",
        "\\end{array}\n",
        "\\right.\n",
        "$\n",
        "\n",
        "$\\epsilon$-greedy is a popular choice. Though with a constant $\\epsilon$ it continues to explore, so it still leads to linear expected total regret."
      ],
      "metadata": {
        "id": "IXgjHq1VQROV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Policy Search\n",
        "\n",
        "We can also learn policies directly instead of learning values.\n",
        "For instance, define action preferences $H_t(a)$ and a **softmax** policy,\n",
        "\n",
        "$\n",
        "\\pi(a) = \\frac{e^{H_t(a)}}{\\sum_b e^{H_t(b)}}\n",
        "$\n",
        "\n",
        "These action preferences are not values, they are just learnable policy parameters.\n",
        "\n",
        "Note that **softmax** is a generalization of **standard logistic function**\n",
        "$f(x) = \\frac{1}{1 + e^{-x}} = \\frac{e^x}{1 + e^x}, \\frac{d}{dx}f(x) = f(x) \\cdot (1 - f(x))$"
      ],
      "metadata": {
        "id": "2kvOdEI1mSFw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Policy gradients\n",
        "\n",
        "\n",
        "In the bandit case, we want to update the current policy paramters $\\theta_t$ as follows:\n",
        "\n",
        "$\n",
        "\\theta_{t+1} = \\theta_t + \\alpha \\nabla_\\theta E[R_t | \\pi_\\theta]\n",
        "$\n",
        "\n",
        "$\\theta_t$ can be thought of as a vector that contains just the action preferences. This can also be scaled to larger problems where $\\theta_t$ represent parameters of deep neural networks.\n",
        "\n",
        "The gradient of expectations is not directly known or computable - so we need some stochastic sample of this.\n",
        "\n",
        "Note that this gradient is also not computable directly from samples of the reward distribution for a given action - as it depends on the softmax policy parameters."
      ],
      "metadata": {
        "id": "_uxC9xoFnec9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyzing the error term $ \\nabla_\\theta E[R_t | \\pi_\\theta],\n",
        "$ <br/> $\n",
        "= \\nabla_\\theta \\sum_a \\pi_\\theta(a) q(a)\n",
        "$ <br/> $\n",
        "= \\sum_a q(a) \\nabla_\\theta \\pi_\\theta(a)\n",
        "$ <br/> $\n",
        "= E\\left[\\frac{R_t}{\\pi_\\theta(A_t)} \\nabla_\\theta \\pi_\\theta(A_t) \\right]\n",
        "= E\\left[R_t \\nabla_\\theta log(\\pi_\\theta(A_t)) \\right]\n",
        "$\n"
      ],
      "metadata": {
        "id": "OIs0tdLVo7zY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For softmax, we need to deal with two cases,\n",
        "<br/> $\n",
        "\\frac{\\delta}{\\delta x}f(x, x)\n",
        "= \\frac{\\delta}{\\delta x} \\frac{e^x}{e^x + \\sum_{y \\neq x} e^y}\n",
        "= \\frac{e^x \\sum_{y \\neq x} e^y}{\\left(e^x + \\sum_{y \\neq x} e^y \\right)^2}\n",
        "= f(x, x) (1 - f(x, x))\n",
        "\\implies\n",
        "\\frac{\\frac{\\delta}{\\delta x}f(x, x)}{f(x, x)} = 1 - f(x, x)\n",
        "$ <br/> $\n",
        "\\frac{\\delta}{\\delta x}f(w, x)\n",
        "= \\frac{\\delta}{\\delta x} \\frac{e^w}{e^x + \\sum_{y \\neq x} e^y}\n",
        "= \\frac{-e^w e^x}{\\left(e^x + \\sum_{y \\neq x} e^y \\right)^2}\n",
        "= -f(w, x) f(x, x)\n",
        "\\implies\n",
        "\\frac{\\frac{\\delta}{\\delta x}f(w, x)}{f(w, x)} = - f(x, x)\n",
        "$\n",
        "\n",
        "Using the above results, we can update the parameters $\\theta = H_t$ as follows,\n",
        "<br/> $\n",
        "H_{t+1}(a) = H_{t}(a) + \\alpha R_{t} (I(A_t = a) - \\pi_t(a))\n",
        "$\n",
        "\n"
      ],
      "metadata": {
        "id": "wRIimUQB2R_f"
      }
    }
  ]
}