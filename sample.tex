hl|tc| Mean and Variance
The Mean \(E[X]\) and Variance \(V[X]\) are numbers that measure the central value and spread of the distribution of a given random variable \(X\).
tc| \(E[X]=\sum_x xP(X=x)\)
tc| \(V[X] = \sum_x (x - E[X])^2 P(X=x) = E[(X-E[X])^2] = E[X^2] - E[X]^2 \)
Given two numbers \((a, b),\) we can compute (mean, variance) of the scaled and translated random variable \(aX+b\) as follows,
tc| \(E[aX+b] = aE[X]+b\)
tc| \(V[aX+b] = E[(aX+b-E[aX+b])^2] = E[a^2(X-E[X])^2] = a^2V[x]\)
Given two random variables \(X\) and \(Y\), the expectation and variance of \(X+Y\) can be computed with the same basic definitions as well,
tc| \( E[X+Y] = \sum_x \sum_y P(X=x) P(Y=y) (x+y)\)
tc| \( = \sum_x P(X=x) \sum_y P(Y=y) (x+y) \)
tc| \( = \sum_x P(X=x) \{x + E[Y]\} = E[X] + E[Y] \)
tc| \( V[X+Y] = E[(X+Y - E[X+Y])^2] = E[((X-E[X])+(Y-E[Y]))^2] \)
tc| \(= (V[X] + V[Y]) + 2E[(X-E[X])(Y-E[Y])] \)
If the random variables are independent, the covariance term in the variance of summation is zero, and the variance of the sum is the sum of varaince values. Note that variance scales quadratically when a random variable is scaled, but scales linearly when independent copies of the random variable are added to each other. The intuition behind this distinction is that when adding independent identical copies of a random variable, we allow destructive interference in the result which reduces the overall spread. The same does not happen with simple scaling.
We can also simplify these properties for the product of the given random variables assuming independence as follows,
tc| \( E[XY] = \sum_x \sum_y P(X=x)P(Y=y) xy = \sum_x P(X=x)x \sum_y P(Y=y)y = E[X]E[Y] \)
tc| \( V[XY] = E\left[ (XY-E[XY])^2 \right] = E\left[X^2Y^2 + E[X]^2 E[Y]^2 - 2XYE[X]E[Y] \right] = E[X^2]E[Y^2] - E[X]^2E[Y]^2 \)
tc| \( V[X]V[Y] = E\left[(X-E[X])^2(Y-E[Y])^2 \right] = E[X^2]E[Y^2] - E[X]^2E[Y^2] - E[X^2]E[Y]^2 + E[X]^2E[Y]^2 \)
Using the expansions of \(V[XY]\) and \(V[X]V[Y]\) above, we can observe the following,
tc| \( V[XY] = V[X]V[Y] + E[X^2]E[Y]^2 + E[X]^2E[Y^2] - 2E[X]^2E[Y]^2 \)
tc| \( V[XY] = V[X]V[Y] + V[X]E[Y]^2 + V[Y]E[X]^2 \)
hl|tc| Deviation Bounds
Chebyshev and Markov's inequalities provide upper bound on probabily of getting results at least as extreme as a given lower bound. Markov's inequality applies to only non-negative random variables, and is a statement in terms of mean. Chebyshev's inequality does not assume non-negative valued variable, and is a statement in terms of the variance of the distribution.
Let \(X\) be a non-negative random variable with mean \(\mu=E[X]\), and a given positive lower bound \(a\). Proof sketch of Markov's inequality is as follows,
tc| \( \mu = \sum_x xP(X=x) \ge \sum_{x\ge a}xP(X=x) \ge \sum_{x\ge a} aP(X=x) \implies P(X\ge a) \le \frac{\mu}{a} \)
Similarly, Let \(X\) be a random variable with mean \(\mu=E[X]\), variance \(\sigma=V[X]\), and a given positive lower bound \(a\). Proof sketch of Chebyshev's inequality is as follows,
tc| \( \sigma^2 = \sum_x (x - \mu)^2 P(X=x) \ge \sum_{|x-\mu| \ge a} (x - \mu)^2 P(X=x) \ge \sum_{|x-\mu|} a^2 P(X=x) \implies P(|x - \mu| \ge a) \le \frac{\sigma^2}{a^2} \)
hl|tc| Law of Large Numbers
Given samples taken from inpendent identical copies of a random variable - the law of large number states that as sample size grows, the sample mean converges to the mean of the underlying distribution in a probabilistic manner. We can prove [weak] law of large numbers using Chebyshev's inequality.
Let \(X_1,X_2,..,X_n\) be independent identical copies of random variable \(X\) with mean \(\mu=E[X]\) and variance \(\sigma^2=V[X]\). We are interested in the properties of the distribution of sample mean \(\ \bar{X} = \frac{\sum_{i=1}^n X_i}{n} \).
tc| \( E[\bar{X}] = E \left[\frac{\sum_{i=1}^nX_i}{n} \right] = \frac{1}{n} E\left[\sum_{i=1}^n X_i \right] = \frac{1}{n} n\mu = \mu \)
tc| \( V[\bar{X}] = V\left[\frac{ \sum_{i=1}^n X_i}{n} \right] = \frac{1}{n^2} V\left[\sum_{i=1}^n X_i \right] = \frac{1}{n^2} n\sigma^2 = \frac{\sigma^2}{n} \)
We can already observe that sampling results converge to a degenerate distribution focused on the mean of the underlying variable with zero variance as the sample size grows. Chebyshev's inequality can be used to make this statement more precise.
tc| \( P(|\bar{x} -\mu| \ge \epsilon) \le \frac{\sigma^2}{n\epsilon^2} \implies n \rightarrow \infty \Rightarrow \bar{x} \rightarrow \mu \)
hl|tc| Hoeffding's inequalities
Hoeffding's inequalities can result in much faster convergence compared to the plain application of Chebyshev's inequality. For a basic case, we consider a random variable \(X\) which can assume values in {1, -1} with equal probability. Applying Chebyshev's inequality on samples taken from this, we observe that probability of finding values above a lower bound is inversly proportional to the sample size.
tc| \( P(\sum_{i=1}^n X_i \ge na) \le \frac{1}{na^2} \)
We can analyze the same with Markov's inequality applied to scaled exponentiated version of the random variable lower bound noted above,
tc| \( P(\sum_{i=1}^n X_i \ge na) \iff P\left(e^{s\sum_{i=1}^nX_i} \ge e^{sna}\right) \)
tc| \( P\left(e^{s\sum_{i=1}^n X_i} \ge e^{sna} \right) \le \frac{E\left[e^{s\sum_{i=1}^n X_i}\right]}{e^{sna}} = \left(\frac{E\left[ e^sX \right]}{e^{sa}} \right)^n = \left( \frac{e^s + e^{-s}}{2e^{sa}} \right)^n \)
The two terms of interest on the last fraction are \(\frac{e^s+e^{-s}}{2}\) and \(e^{sa}\). We can manipulate the arbitrary scaling factor to try to minimize this ratio. At \(s=0\), both of these are \(1\) with derivatives \(0,1\) respectively. Thus, we can always find values of \(s\) to make the entire expression less than \(1\) which would decay exponentially with \(n\).
Expanding the taylor series for the numerator,
tc| \( \frac{e^s+e^{-s}}{2} = \sum_{i=0}^\infty \frac{s^{2i}}{(2i)!} \le \sum_{i=0}^\infty \frac{\left(s^2\right)^i}{2^ii!} = \sum_{i=0}^\infty \frac{\left(\frac{s^2}{2}\right)^i}{i!} = e^\frac{s^2}{2}\)
Substituting \(s=a\) and the last result in the previous results, we obtain the desired result. The following inequality shows that for the given structure, the probability of the sample mean deviating beyond a certain bound from the random variable mean decays exponentially with sample size.
hl|tc| \( P(\sum_{i=1}^n X_i \ge na) \le e^{\frac{-na^2}{2}} \)
hl|tc| de Moivre - Laplace CLT
A special of central limit theorem is the convergence of Binomial to Normal distribution. Binomial distribution can be thought of as sampling distribution with a number of identical independently distributed boolean random variables.
If we start with a Binomial Distribution Symmetric around its mean and standardize it, we can easily show that for a large value of \(n\), the dominating terms of the distribution are identical to the standard Normal using a series of two approximations.
Symmetric Binomial Distribution, Sterling's approximation, and Taylor's series for natual logarithm are defined as follows,
tc| \( P(k) = \frac{1}{2^n} \frac{n!}{k!(n-k)!}, \mu = n/2, \sigma^2 = n/4 \)
tc| \( \ln(n!) = \sum_{i=1}^{n} \ln(i) \approx \int^n_1 \ln(x) dx \approx n \cdot \ln(n) - n \Rightarrow n! \approx n^n e^{-n} \)
tc| \( \ln(1+x) = \sum_{i=1}^{\infty} {(-1)}^{i-1}\frac{x^i}{i} \approx x - \frac{x^2}{2}, 0 \lt x \ll 1 \)
Applying Stirling's Approximation to symmetric Binomial definition, we get the following,
tc| \( \approx \frac{1}{2^n} \frac{n^n e^{-n}}{k^k e^{-k} (n-k)^{n-k} e^{-(n-k)}} \)
tc| \( \Rightarrow \frac{1}{2^n} \frac{n^n}{k^k(n-k)^{n-k}} \)
tc| \( \Rightarrow \left(\frac{n/2}{k}\right)^k \left(\frac{n/2}{n-k}\right)^{n-k} \)
tc| \( \Rightarrow \exp\left\{ k \cdot \ln\left(\frac{n/2}{k}\right) + (n-k) \cdot \ln\left(\frac{n/2}{n-k}\right) \right\} \)
Let \(x = \frac{k - n/2}{\sqrt{n}/2}\) and approximate natural logarithm using Taylor's series,
tc| \( \Rightarrow \exp\left\{\frac{1}{2}\left(-(n + x\sqrt{n}) \cdot \ln\left(1+\frac{x}{\sqrt{n}}\right) + -(n - x\sqrt{n}) \cdot \ln\left(1-\frac{x}{\sqrt{n}}\right) \right)\right\} \)
tc| \( \approx \exp\left\{\frac{1}{2}\left(-(n + x\sqrt{n}) \cdot \left(\frac{x}{\sqrt{n}} - \frac{x^2}{2n}\right) + -(n - x\sqrt{n}) \cdot \left(-\frac{x}{\sqrt{n}} - \frac{x^2}{2n}\right) \right)\right\} \Rightarrow e^{-\frac{x^2}{2}} \)
hs|tc| \( \therefore P(k) \approx e^{-\frac{x^2}{2}} \)
Note that this function is defined by the following core property,
hs|tc| \( \frac{d}{d\alpha}f(\alpha)= -\alpha \cdot f(\alpha), \)
If we have non-zero mean and non-unit variance, the following generalized version holds,
hs|tc| \( \frac{d}{d\alpha}f(\alpha)= -\frac{\alpha-\mu}{\sigma^2} \cdot f(\alpha) \)
We can show that the same holds for a Binomial distribution as follows,
tc| \( P(k+1) - P(k) = \frac{n!}{(k+1)!(n-(k+1))!} - \frac{n!}{k!(n-k)!} \)
tc| \( \Rightarrow P(k)\left(\frac{n-k}{k+1} - 1\right) \Rightarrow P(k)\frac{n-2k-1}{k+1} \)
tc| \( \Rightarrow P(k)\frac{n-2(n/2 + x\sqrt{n}/2)-1}{n/2 + x\sqrt{n}/2 +1} \Rightarrow -P(k)\frac{x\sqrt{n} + 1}{n/2 + x\sqrt{n}/2 + 1} \approx -\frac{x}{\sqrt{n}/2}P(k)\frac{xn/2}{xn/2} \)
hs|tc| \( \therefore P(k+1)-P(k) \rightarrow -\frac{k-n/2}{n/4}P(k) \)
hl|tc| Lindeberg-Levy CLT
The general CLT focuses on Samples taken from any probability distribution with finite mean and variance. By analysing characterstic function or moment-generating function, it can be seen that the structure approaches the Normal Distribution.
Note that the Characterstic function of a distribution is analogous to Fourier Transform and MGF is a variant of the same which abstracts away the process of computing \(n^{th}\) moment of the distribution.
Assuming \({X_1,X_2,..X_n}\) are independent identical random variables with finite mean \(\mu\) and variance \(\sigma^2\),
tc| \( Z_n = \sum_{i=1}^{n}\frac{X_i - \mu}{\sqrt{n}\sigma} = \sum_{i=1}^{n}\frac{Y_i}{\sqrt{n}}, E[Y_i] = 0, V[Y_i] = 1 \)
Focusing on the characterstic function of \(Z_n\),
tc| \( \phi_{Z_n}(t) = \phi_{\sum_{i=1}^{n}Y_i/\sqrt{n}}(t) = \phi_{Y_1}(t/\sqrt{n})^n \)
tc| \( \Rightarrow \phi_{Z_n}(t) = (1 + E[Y_1]\cdot it/\sqrt{n} - V[Y_1]\cdot t^2/2n + \epsilon)^n \)
hs|tc| \( \phi_{Z_n}(t) \approx e^{\frac{-t^2}{2}} \)
Focusing on the characterstic function of standard normal distribution,
tc| \( \phi_{N(0,1)} = \int \frac{1}{\sqrt{2\pi}} e^{\frac{-x^2}{2}} e^{itx} dx = \frac{1}{\sqrt{2\pi}} \int e^{itx - \frac{x^2}{2}} dx = \frac{1}{\sqrt{2\pi}} \int e^{-\frac{1}{2} (x^2 - 2itx + (it)^2)} dx \)
tc| \( \Rightarrow \phi_{N(0,1)} = e^{\frac{-t^2}{2}} \frac{1}{\sqrt{2\pi}} \int e^{-\frac{1}{2} (x - it)^2} dx \)
hs|tc| \( \phi_{N(0,1)} = e^{\frac{-t^2}{2}} \)
hs|tc| \( \therefore Z_n \rightarrow N(0, 1) \)
hl|tc| Total Variation distance
Total Variation (TV) distance of two probability distributions over domain \(\Omega\) is defined as the maximum absolute difference in probability of any given event.
tc| \( \delta(P, Q) = \sup_{S \ \subset \ \Omega} | P(S) - Q(S) | \)
The supremum is obtained by including all points where one of the distributions is strictly larger than the other. The distance is symmetric because the total difference when either of the distributions is larger must sum up to one with the common area.
tc| \( \delta(P, Q) = \frac{1}{2} \sum_{s \in \Omega} |P(s) - Q(s)| = \sum_{s \in \Omega} \max(0, P(s) - Q(s)) = \sum_{s \in \Omega} \max(0, Q(s) - P(s)) \)
hl|tc| Kullback-Leibler divergence
The information content (entropy) of a probability distribution is defined as \( \sum_{s \in \Omega} P(s) \log(P(s)) \). This value represents the expected number of symbols required per message for processing values from the given distribution.
Kullback-Leibler (KL) divergence of two distributions is defined in terms of the relative information content of two distributions. It measures the additional expected number of symbols per message when processing the underlying distribution \(P\) using model fit for distribution \(Q\). It can also be thought of as the information gain achieved if we use the model \(P\) instead of \(Q\).
tc| \( D_{KL} (P \ || \ Q) = \sum_{s \in \Omega} P(s) \log \left(\frac{P(s)}{Q(s)}\right) \)
hl|tc| Pinsker's Inequality
Pinsker's Inequality establishes a relationship between TV distance & KL divergence of two distributions,
tc| \( D_{KL} (P \ || \ Q) \ \ge \ 2 \ \delta(P, Q)^2 \)
The following is a proof sketch based on Yihong Wu's lecture notes. First we establish this property for binary random variables \( B(p), B(q) \),
tc| \( p \log{ \left( \frac{p}{q} \right) } + (1-p) \log{ \left( \frac{1-p}{1-q} \right) \ge 2(p-q)^2 } \)
For this, we focus on the function \( f(x) = p \log (x) + (1 - p) \log (1 - x) \). We can observe that the L.H.S. on the previous inequality is equivalent to \( f(p) - f(q) \).
tc| \( f(p) - f(q) = \int_q^p f'(x) \ dx = \int_q^p \frac{p - x}{x (1 - x)} dx \ge 4 \int_q^p (p - x) dx \)
The last inequality stems from the fact that parabola \( x (1-x) \) stays below \( \frac{1}{4} \) for all real numbers. This can be easily checked with the derivatives of the same. We now have,
tc| \( f(p) - f(q) \ge 4 \left[ \frac{-(p-x)^2}{2} \right]_q^p = 2 (p - q)^2 \)
The above property holds for binary random variables defined over any subset of the domain \( \Omega \). Considering the subset \( S \subset \Omega \) which gives us TV distance Supremum, we get the following,
tc| \( D_{KL}(P(S) \ || \ Q(S)) \ge 2(P(S)-Q(S))^2 = 2 \ \delta(P,Q)^2 \)
The last part of the proof involves the Data Processing Inequality. This is a property all meaningful divergence measurements are supposed to have. Intuitively, it states that post-processing a distribution can not increase the information content. For the context above, it states that KL divergence over a subset of the domain must be less than or equal to the divergence for the full domain.
To show Data Processing Inequality for KL divergence, we first build Log-sum inequality from Jensen's inequality,
li| Jensen's Inequality : \( E[f(X)] \ge f(E[X]) \) where \(f\) is a convex function and \(E\) is a convex combination.
li| Log sum Inequality : For \(a_i \ge 0\), \(b_i \ge 0\), \(a = \sum_i a_i\), \(b = \sum_i b_i\), we obtain the following result,
tc| \( \sum_i a_i \log \left( \frac{a_i}{b_i} \right) = b \sum_i \frac{b_i}{b} \left( \frac{a_i}{b_i} \log \left( \frac{a_i}{b_i} \right) \right) \ge b \left( \left( \sum_i \frac{b_i}{b} \frac{a_i}{b_i} \right) \log \left( \sum_i \frac{b_i}{b} \frac{a_i}{b_i} \right) \right) = a \log \left( \frac{a}{b} \right) \)
Applying this to KL divergence of post-processed distributions \(P\) & \(Q\), we get the following,
tc| \( \begin{align} D_{KL}(W \cdot P \ || \ W \cdot Q) & = \sum_y \left(\sum_x W(y|x) P(x) \right) \log \left( \frac{ \sum_x W(y|x) P(x) }{ \sum_x W(y|x) Q(x) } \right) \\ & \le \sum_y \left( \sum_x W(y|x) P(x) \ \log \left( \frac{ W(y|x) P(x) }{ W(y|x) Q(x) } \right) \right) \\ & \le \sum_x P(x) \log \left( \frac{P(x)}{Q(x)} \right) = D_{KL}(P \ || \ Q) \end{align} \)
This completes the proof for Pinsker's Inequality with the comparison over TV distance supremum \(S \subset \Omega\),
hs|tc| \( D_{KL}(P \ || \ Q) \ge D_{KL}( P(S) \ || \ Q(S)) \ge 2(P(S)) - Q(S))^2 = 2 \ \delta(P, Q)^2 \)