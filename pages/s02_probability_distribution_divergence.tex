hl|tc| Total Variation distance
Total Variation (TV) distance of two probability distributions over domain \(\Omega\) is defined as the maximum absolute difference in probability of any given event.
tc| \( \delta(P, Q) = \sup_{S \ \subset \ \Omega} | P(S) - Q(S) | \)
The supremum is obtained by including all points where one of the distributions is strictly larger than the other. The distance is symmetric because the total difference when either of the distributions is larger must sum up to one with the common area.
tc| \( \delta(P, Q) = \frac{1}{2} \sum_{s \in \Omega} |P(s) - Q(s)| = \sum_{s \in \Omega} \max(0, P(s) - Q(s)) = \sum_{s \in \Omega} \max(0, Q(s) - P(s)) \)
hl|tc| Kullback–Leibler divergence
The information content (entropy) of a probability distribution is defined as \( \sum_{s \in \Omega} P(s) \log(P(s)) \). This value represents the expected number of symbols required per message for processing values from the given distribution.
Kullback–Leibler (KL) divergence of two distributions is defined in terms of the relative information content of two distributions. It measures the additional expected number of symbols per message when processing the underlying distribution \(P\) using model fit for distribution \(Q\). It can also be thought of as the information gain achieved if we use the model \(P\) instead of \(Q\).
tc| \( D_{KL} (P \ || \ Q) = \sum_{s \in \Omega} P(s) \log \left(\frac{P(s)}{Q(s)}\right) \)
hl|tc| Pinsker's Inequality
Pinsker's Inequality establishes a relationship between TV distance & KL divergence of two distributions,
tc| \( D_{KL} (P \ || \ Q) \ \ge \ 2 \ \delta(P, Q)^2 \)
The following is a proof sketch based on Yihong Wu's lecture notes. First we establish this property for binary random variables \( B(p), B(q) \),
tc| \( p \log{ \left( \frac{p}{q} \right) } + (1-p) \log{ \left( \frac{1-p}{1-q} \right) \ge 2(p-q)^2 } \)
For this, we focus on the function \( f(x) = p \log (x) + (1 - p) \log (1 - x) \). We can observe that the L.H.S. on the previous inequality is equivalent to \( f(p) - f(q) \).
tc| \( f(p) - f(q) = \int_q^p f'(x) \ dx = \int_q^p \frac{p - x}{x (1 - x)} dx \ge 4 \int_q^p (p - x) dx \)
The last inequality stems from the fact that parabola \( x (1-x) \) stays below \( \frac{1}{4} \) for all real numbers. This can be easily checked with the derivatives of the same. We now have,
tc| \( f(p) - f(q) \ge 4 \left[ \frac{-(p-x)^2}{2} \right]_q^p = 2 (p - q)^2 \)
The above property holds for binary random variables defined over any subset of the domain \( \Omega \). Considering the subset \( S \subset \Omega \) which gives us TV distance Supremum, we get the following,
tc| \( D_{KL}(P(S) \ || \ Q(S)) \ge  2(P(S)-Q(S))^2 = 2 \ \delta(P,Q)^2 \)
The last part of the proof involves the Data Processing Inequality. This is a property all meaningful divergence measurements are supposed to have. Intuitively, it states that post-processing a distribution can not increase the information content. For the context above, it states that KL divergence over a subset of the domain must be less than or equal to the divergence for the full domain.
To show Data Processing Inequality for KL divergence, we first build Log-sum inequality from Jensen's inequality,
li| Jensen's Inequality : \( E[f(X)] \ge f(E[X]) \) where \(f\) is a convex function and \(E\) is a convex combination.
li| Log sum Inequality : For \(a_i \ge 0\), \(b_i \ge 0\), \(a = \sum_i a_i\), \(b = \sum_i b_i\), we obtain the following result,
tc| \( \sum_i a_i \log \left( \frac{a_i}{b_i} \right) = b \sum_i \frac{b_i}{b} \left( \frac{a_i}{b_i} \log \left( \frac{a_i}{b_i} \right) \right) \ge b \left( \left( \sum_i \frac{b_i}{b} \frac{a_i}{b_i} \right) \log \left( \sum_i \frac{b_i}{b} \frac{a_i}{b_i} \right) \right) = a \log \left( \frac{a}{b} \right) \)
Applying this to KL divergence of post-processed distributions \(P\) & \(Q\), we get the following,
tc| \( \begin{align} D_{KL}(W \cdot P \ || \ W \cdot Q) & = \sum_y \left(\sum_x W(y|x) P(x) \right) \log \left( \frac{ \sum_x W(y|x) P(x) }{ \sum_x W(y|x) Q(x) } \right) \\ & \le \sum_y \left( \sum_x W(y|x) P(x) \ \log \left( \frac{ W(y|x) P(x) }{ W(y|x) Q(x) } \right) \right) \\ & \le \sum_x P(x) \log \left( \frac{P(x)}{Q(x)} \right) = D_{KL}(P \ || \ Q) \end{align} \)
This completes the proof for Pinsker's Inequality with the comparison over TV distance supremum \(S \subset \Omega\),
hs|tc| \( D_{KL}(P \ || \ Q) \ge D_{KL}( P(S) \ || \ Q(S)) \ge 2(P(S)) - Q(S))^2 = 2 \ \delta(P, Q)^2 \)