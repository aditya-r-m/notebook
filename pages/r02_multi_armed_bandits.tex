hl|tc| Multi-Armed Bandits
A Multi-Armed Bandits is a set of reward distributions per allowed action : \( \{ R_a \ | \ a \in A \} \). In this simplified setting, actions do not have long term consequences and all observations except the reward can be ignored. The goal is to maximize the cumulative reward \( \sum_{i=1}^t R_i \) by learning a policy, which is a distribution on \( A \). We start with the following definitions,
li| Action value : \( q(a) = E[R_t | A_t = a] \)
li| Optimal value : \( v_* = max_{a \in A} \ q(a) \)
li| Regret of an action : \( \Delta_a = v_* - q(a) \)
The goal of maximizing the cumulative reward can equivalently be stated as minimizing the total regret : \( L_t = \sum_{i=1}^t \Delta_{A_i} \)
The following algorithms will be explored in the rest of this document,
li| Greedy
li| \(\epsilon\)-Greedy
li| UCB
li| Thompson sampling
li| Policy gradients
The first 3 of the above algorithms realy on Action Value Estimates. A simple estimate can be defined as the average of all sampled Action Values.
li| Indicator function of Action \(a\) : \( I(A_i = a) \) as \( I(true) = 1 \) and \( I(false) = 0 \)
li| Count of Action \(a\) : \( N_t(a) = \sum_{i=1}^t I(A_i = a) \)
li| Sample Average of Action \(a\) : \( Q_t(a) = \frac{1}{N_t(a)} \sum_{i=1}^t I(A_i = a) R_i \)
The Sample Average can be updated incrementally,
tc| \( Q_t(a) = \frac{1}{N_{t}(a)} (N_{t-1}(a) \ Q_{t-1}(a) \ + \ I(A_t = a) \ R_t ) \)
tc| \( = \begin{cases} \ Q_t(a) = Q_{t-1}(a) + \frac{1}{N_t(a)}(R_t - Q_{t-1}(a)) & \text{if } A_t=a \\ \ Q_t(a) = Q_{t-1}(a) & \text{otherwise.} \end{cases} \)
Note that in the last equation, the ratio \( \alpha_t = \frac{1}{N_t(a)} \) is the learning rate that controls step sizes. Different configurations are possible for this ratio for learning the Value Estimates, for example, a constant value will lead to tracking instead of averaging.
hs| Greedy Policy
Select the action with the largest Value Estimate : \( \pi_t(a) = I(argmax_{a_i} \ Q_t(a_i) = a) \)
The regret for Greedy Policy grows linearly because the initial Action Value Estimates can deviate significantly from true Action Values, causing Greedy Policy to fix a suboptimal action as the preferred one.
hs| \(\epsilon\)-Greedy Policy
Select random action with a small probability and the action with the largest Value Estimate otherwise.
tc| \( \pi(a) = \begin{cases} \ (1 - \epsilon) \ + \ \frac{\epsilon}{|A|} & argmax_{a_i} \ Q_t(a_i)=a \\ \ \frac{\epsilon}{|A|} & \text{otherwise.} \end{cases} \)
The regret for \(\epsilon\)-Greedy Policy with constant \(\epsilon\) grows linearly, since sub-optimal actions will be chosen with a static probability.
hs| Policy Gradients
Policy Gradient ascent approach requires a policy function definition \(\pi_\theta(a)\) with parameters \(\theta\). These parameters are not necessarily equal to action values, but we update these in a manner that prioritizes actions with higher rewards. To update the policy, we compute the rate of change of expected reward w.r.t. \(\theta\), and move the parameters in the direction where reward growth is the steepest.
tc| \( \begin{aligned} \nabla_\theta E[R_t | \pi_\theta] &= \nabla_\theta \sum_a \pi_\theta(a) \ q(a) \\ &= \sum_a q(a) \ \pi_\theta(a) \frac{\nabla_\theta \pi_\theta(a)}{\pi_\theta(a)} \\ &= E\left[R_t \ \frac{\nabla_\theta \pi_\theta(a)}{\pi_\theta(a)} \right] = E\left[ R_t \ \nabla_\theta \log(\pi_\theta(a)) \right] \end{aligned} \)
The intuition behind the division by \(\pi_\theta(a)\) in the last line is as follows : The Stochastic Gradient ascent requires policy correction by \(\sum_a q(a) \ \nabla_\theta \pi_\theta(a) \). Since we do not know the true action values \(q(a)\), we must sample rewards to estimate the correction term as we take actions using the same policy \(\pi_\theta(a)\). This iterative correction step occurs in a non-uniform way over the action set depending on the existing policy. Therefore, diving the current correction term by the probability of the current action being chosen results in an overall uniform correction over the action set.
hl|tc| \( \theta_{t+1} = \theta_t + \alpha R_t \ \nabla_\theta \log(\pi_\theta(A_t)) \)
We can define a softmax policy using parameters \(H_t(a)\) as follows, where the iteration guarantees that preferences for actions with higher rewards increase due to larger \(R_t\) for the positive case and smaller \(R_t\) for the negative case.
hl|tc| \( \pi(a) = \frac{e^{H_t(a)}}{\sum_x e^{H_t(x)}} \implies \frac{\delta \pi(A_t)}{\delta H_t(a)} = \begin{cases} \pi(a) \ \cdot \ (1 - \pi(a)), & A_t=a \\ - \pi(a)^2, & otherwise \end{cases} \)
hl|tc| \( \therefore H_{t+1}(a) = H_t(a) + \alpha R_t \left[I(A_t=a) - \frac{e^{H_t(a)}}{\sum_x e^{H_t(x)}}\right] \)
Note that the total expected regret with this approach can still be linear in the number of time-steps, because the iterations may converge towards a local optimum with non-zero regret.
TODO : Gradient Bandits with baseline
TODO : Lai-Robbins lower bound
TODO : UCB
